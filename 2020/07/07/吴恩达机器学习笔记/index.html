<!DOCTYPE html>
<html lang="zh-CN">
<head>

 <!--pjax：防止跳转页面音乐暂停-->
  <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 

  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta name="description" content="一.引言（Introduction)1.1什么是机器学习第一个机器学习定义来源于Arthur Samuel,他定义为： 在特定的编程情况下，给予计算机学习能力。另一个年代近一点的定义，由   Tom Mitchell 提出，来自卡内基梅隆大学，Tom 定义的机器学习是，一个好的学习问题定义如下，他说，一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验 E 后，经">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达机器学习笔记">
<meta property="og:url" content="http://example.com/2020/07/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Grit&#39;s blog">
<meta property="og:description" content="一.引言（Introduction)1.1什么是机器学习第一个机器学习定义来源于Arthur Samuel,他定义为： 在特定的编程情况下，给予计算机学习能力。另一个年代近一点的定义，由   Tom Mitchell 提出，来自卡内基梅隆大学，Tom 定义的机器学习是，一个好的学习问题定义如下，他说，一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验 E 后，经">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20210216224108606.png">
<meta property="og:image" content="http://example.com/images/image-20210216225057126.png">
<meta property="og:image" content="http://example.com/images/image-20210216225452470.png">
<meta property="og:image" content="http://example.com/images/image-20210216225824075.png">
<meta property="og:image" content="http://example.com/images/image-20210216232328225.png">
<meta property="og:image" content="http://example.com/images/image-20210216232438960.png">
<meta property="og:image" content="http://example.com/images/image-20210216232605293.png">
<meta property="og:image" content="http://example.com/images/image-20210216234140487.png">
<meta property="og:image" content="http://example.com/images/image-20210216234256392.png">
<meta property="og:image" content="http://example.com/images/image-20210216234547570.png">
<meta property="og:image" content="http://example.com/images/image-20210216235141468.png">
<meta property="og:image" content="http://example.com/images/image-20210216235152322.png">
<meta property="og:image" content="http://example.com/images/image-20210216235528290.png">
<meta property="og:image" content="http://example.com/images/image-20210216235547767.png">
<meta property="og:image" content="http://example.com/images/image-20210216235758941.png">
<meta property="og:image" content="http://example.com/images/image-20210217000026099.png">
<meta property="og:image" content="http://example.com/images/image-20210217000338121.png">
<meta property="og:image" content="http://example.com/images/image-20210217000401134.png">
<meta property="og:image" content="http://example.com/images/image-20210217002837122.png">
<meta property="og:image" content="http://example.com/images/image-20210217000534228.png">
<meta property="og:image" content="http://example.com/images/image-20210217000553809.png">
<meta property="og:image" content="http://example.com/images/image-20210217002957464.png">
<meta property="og:image" content="http://example.com/images/image-20210217003019388.png">
<meta property="og:image" content="http://example.com/images/image-20210217003035133.png">
<meta property="og:image" content="http://example.com/images/image-20210217002906737.png">
<meta property="og:image" content="http://example.com/images/image-20210217135149679.png">
<meta property="og:image" content="http://example.com/images/image-20210217135313151.png">
<meta property="og:image" content="http://example.com/images/image-20210217135736845.png">
<meta property="og:image" content="http://example.com/images/image-20210217135935284.png">
<meta property="og:image" content="http://example.com/images/image-20210217140013223.png">
<meta property="og:image" content="http://example.com/images/image-20210217140116234.png">
<meta property="og:image" content="http://example.com/images/image-20210217140159797.png">
<meta property="og:image" content="http://example.com/images/image-20210217141700534.png">
<meta property="og:image" content="http://example.com/images/image-20210217141916558.png">
<meta property="og:image" content="http://example.com/images/image-20210217141936436.png">
<meta property="og:image" content="http://example.com/images/image-20210217141947358.png">
<meta property="og:image" content="http://example.com/images/image-20210217142148319.png">
<meta property="og:image" content="http://example.com/images/image-20210217142153897.png">
<meta property="og:image" content="http://example.com/images/image-20210217142200423.png">
<meta property="og:image" content="http://example.com/images/image-20210217144009670.png">
<meta property="og:image" content="http://example.com/images/image-20210217144026104.png">
<meta property="og:image" content="http://example.com/images/image-20210217144136166.png">
<meta property="og:image" content="http://example.com/images/image-20210217144228881.png">
<meta property="og:image" content="http://example.com/images/image-20210217144303006.png">
<meta property="og:image" content="http://example.com/images/image-20210217144426235.png">
<meta property="og:image" content="http://example.com/images/image-20210218161127888.png">
<meta property="og:image" content="http://example.com/images/image-20210218161153908.png">
<meta property="og:image" content="http://example.com/images/image-20210218161321350.png">
<meta property="og:image" content="http://example.com/images/image-20210218161337520.png">
<meta property="og:image" content="http://example.com/images/image-20210218161351222.png">
<meta property="og:image" content="http://example.com/images/image-20210218161449597.png">
<meta property="og:image" content="http://example.com/images/image-20210218161519095.png">
<meta property="og:image" content="http://example.com/images/image-20210218161533113.png">
<meta property="og:image" content="http://example.com/images/image-20210218161643826.png">
<meta property="og:image" content="http://example.com/images/image-20210218161726641.png">
<meta property="og:image" content="http://example.com/images/image-20210218161742466.png">
<meta property="og:image" content="http://example.com/images/image-20210218162307180.png">
<meta property="og:image" content="http://example.com/images/image-20210218162409718.png">
<meta property="og:image" content="http://example.com/images/image-20210218162601709.png">
<meta property="og:image" content="http://example.com/images/image-20210218162658334.png">
<meta property="og:image" content="http://example.com/images/image-20210218162722849.png">
<meta property="og:image" content="http://example.com/images/image-20210218162755926.png">
<meta property="og:image" content="http://example.com/images/image-20210218162804677.png">
<meta property="og:image" content="http://example.com/images/image-20210218162820290.png">
<meta property="og:image" content="http://example.com/images/image-20210218162916440.png">
<meta property="og:image" content="http://example.com/images/image-20210218162943804.png">
<meta property="og:image" content="http://example.com/images/image-20210219205008736.png">
<meta property="og:image" content="http://example.com/images/image-20210219205031615.png">
<meta property="og:image" content="http://example.com/images/image-20210219205335962.png">
<meta property="og:image" content="http://example.com/images/image-20210219205424685.png">
<meta property="og:image" content="http://example.com/images/image-20210219205431707.png">
<meta property="og:image" content="http://example.com/images/image-20210219205839015.png">
<meta property="og:image" content="http://example.com/images/image-20210219205921075.png">
<meta property="og:image" content="http://example.com/images/image-20210219205956276.png">
<meta property="og:image" content="http://example.com/images/image-20210219210047848.png">
<meta property="og:image" content="http://example.com/images/image-20210219210219689.png">
<meta property="og:image" content="http://example.com/images/image-20210219210322856.png">
<meta property="og:image" content="http://example.com/images/image-20210219213349970.png">
<meta property="og:image" content="http://example.com/images/image-20210219223428799.png">
<meta property="og:image" content="http://example.com/images/image-20210219223559321.png">
<meta property="og:image" content="http://example.com/images/image-20210219223750629.png">
<meta property="og:image" content="http://example.com/images/image-20210219223822684.png">
<meta property="og:image" content="http://example.com/images/image-20210219223838732.png">
<meta property="og:image" content="http://example.com/images/image-20210219223947040.png">
<meta property="og:image" content="http://example.com/images/image-20210220072049273.png">
<meta property="og:image" content="http://example.com/images/image-20210220072158178.png">
<meta property="og:image" content="http://example.com/images/image-20210220072214334.png">
<meta property="og:image" content="http://example.com/images/image-20210220072332215.png">
<meta property="og:image" content="http://example.com/images/image-20210220072402181.png">
<meta property="og:image" content="http://example.com/images/image-20210220072422304.png">
<meta property="og:image" content="http://example.com/images/image-20210220072449078.png">
<meta property="og:image" content="http://example.com/images/image-20210220072503337.png">
<meta property="og:image" content="http://example.com/images/image-20210220072549402.png">
<meta property="og:image" content="http://example.com/images/image-20210220072641331.png">
<meta property="og:image" content="http://example.com/images/image-20210220072807366.png">
<meta property="og:image" content="http://example.com/images/image-20210220072846545.png">
<meta property="og:image" content="http://example.com/images/image-20210220073104414.png">
<meta property="og:image" content="http://example.com/images/image-20210220073132499.png">
<meta property="og:image" content="http://example.com/images/image-20210220073214585.png">
<meta property="og:image" content="http://example.com/images/image-20210220073336344.png">
<meta property="og:image" content="http://example.com/images/image-20210220073534877.png">
<meta property="og:image" content="http://example.com/images/image-20210220073624074.png">
<meta property="og:image" content="http://example.com/images/image-20210220073636614.png">
<meta property="og:image" content="http://example.com/images/image-20210220074848320.png">
<meta property="og:image" content="http://example.com/images/image-20210220074927936.png">
<meta property="og:image" content="http://example.com/images/image-20210220075005570.png">
<meta property="og:image" content="http://example.com/images/image-20210220075041803.png">
<meta property="og:image" content="http://example.com/images/image-20210220075116773.png">
<meta property="og:image" content="http://example.com/images/image-20210220075154821.png">
<meta property="og:image" content="http://example.com/images/image-20210220075217278.png">
<meta property="og:image" content="http://example.com/images/image-20210220075622898.png">
<meta property="og:image" content="http://example.com/images/image-20210220075658606.png">
<meta property="article:published_time" content="2020-07-07T13:54:24.000Z">
<meta property="article:modified_time" content="2021-02-28T15:18:05.262Z">
<meta property="article:author" content="grit">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="MachineLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20210216224108606.png">

<link rel="canonical" href="http://example.com/2020/07/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>吴恩达机器学习笔记 | Grit's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<!-- 樱花特效 -->
  

</head>


<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/xiangruiA" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Grit's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description"> 爱自已，是一生浪漫的开始。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-guestbook">

    <a href="/guestbook/" rel="section"><i class="comment fa-fw"></i>留言板</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="grit">
      <meta itemprop="description" content="身体和灵魂总要有一个在路上">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Grit's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达机器学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-07 21:54:24" itemprop="dateCreated datePublished" datetime="2020-07-07T21:54:24+08:00">2020-07-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-28 23:18:05" itemprop="dateModified" datetime="2021-02-28T23:18:05+08:00">2021-02-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/07/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/07/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">

阅读时长 &asymp;

</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">


      
        <h2 id="一-引言（Introduction"><a href="#一-引言（Introduction" class="headerlink" title="一.引言（Introduction)"></a>一.引言（Introduction)</h2><h3 id="1-1什么是机器学习"><a href="#1-1什么是机器学习" class="headerlink" title="1.1什么是机器学习"></a>1.1什么是机器学习</h3><p>第一个机器学习定义来源于<strong>Arthur Samuel</strong>,他定义为： 在特定的编程情况下，给予计算机学习能力。<br>另一个年代近一点的定义，由   <strong>Tom Mitchell</strong> 提出，来自卡内基梅隆大学，Tom 定义的机器学习是，一个好的学习问题定义如下，他说，一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。我认为经验E 就是程序上万次的自我练习的经验而任务 T 就是下棋。性能度量值 P 呢，就是它在与一些新的对手比赛时，赢得比赛的概率。<strong>简单来说就是从E中学习，解决任务T，达到性能度量值P。并且在经过P评判，和不断学习的过程中，程序在处理T时的性能有所提升.</strong></p>
<a id="more"></a>
<h3 id="1-2监督学习"><a href="#1-2监督学习" class="headerlink" title="1.2监督学习"></a>1.2监督学习</h3><p>我们用一个例子介绍什么是监督学习把正式的定义放在后面介绍。假如说你想预测房价。</p>
<p>前阵子，一个学生从波特兰俄勒冈州的研究所收集了一些房价的数据。你把这些数据画出来，看起来是这个样子：横轴表示房子的面积，单位是平方英尺，纵轴表示房价，单位是千美元。那基于这组数据，假如你有一个朋友，他有一套750平方英尺房子，现在他希望把房子卖掉，他想知道这房子能卖多少钱。</p>
<p>那么关于这个问题，机器学习算法将会怎么帮助你呢？</p>
<p><img src="/images/image-20210216224108606.png" alt="image-20210216224108606"></p>
<p>我们应用学习算法，可以在这组数据中画一条直线，或者换句话说，拟合一条直线，根据这条线我们可以推测出，这套房子可能卖$$150,000$，当然这不是唯一的算法。可能还有更好的，比如我们不用直线拟合这些数据，用二次方程去拟合可能效果会更好。根据二次方程的曲线，我们可以从这个点推测出，这套房子能卖接近$$200,000$。稍后我们将讨论如何选择学习算法，如何决定用直线还是二次方程来拟合。两个方案中有一个能让你朋友的房子出售得更合理。这些都是学习算法里面很好的例子。以上就是监督学习的例子。</p>
<p>可以看出，<strong>监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。</strong>在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做回归问题。我们试着推测出一个连续值的结果，即房子的价格。</p>
<p>一般房子的价格会记到美分，所以房价实际上是一系列离散的值，但是我们通常又把房价看成实数，看成是标量，所以又把它看成一个连续的数值。</p>
<p>回归这个词的意思是，我们在试着推测出这一系列连续值属性。</p>
<p>我再举另外一个监督学习的例子。我和一些朋友之前研究过这个。假设说你想通过查看病历来推测乳腺癌良性与否，假如有人检测出乳腺肿瘤，恶性肿瘤有害并且十分危险，而良性的肿瘤危害就没那么大，所以人们显然会很在意这个问题。</p>
<p><img src="/images/image-20210216225057126.png" alt="image-20210216225057126"></p>
<p>让我们来看一组数据：这个数据集中，横轴表示肿瘤的大小，纵轴上，我标出1和0表示是或者不是恶性肿瘤。我们之前见过的肿瘤，如果是恶性则记为1，不是恶性，或者说良性记为0。</p>
<p>我有5个良性肿瘤样本，在1的位置有5个恶性肿瘤样本。现在我们有一个朋友很不幸检查出乳腺肿瘤。假设说她的肿瘤大概这么大，那么机器学习的问题就在于，你能否估算出肿瘤是恶性的或是良性的概率。用术语来讲，这是一个分类问题。</p>
<p>分类指的是，我们试着推测出离散的输出值：0或1良性或恶性，而事实上在分类问题中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出0、1、2、3。0 代表良性，1 表示第1类乳腺癌，2表示第2类癌症，3表示第3类，但这也是分类问题。</p>
<p>因为这几个离散的输出分别对应良性，第一类第二类或者第三类癌症，在分类问题中我们可以用另一种方式绘制这些数据点。</p>
<p>现在我用不同的符号来表示这些数据。既然我们把肿瘤的尺寸看做区分恶性或良性的特征，那么我可以这么画，我用不同的符号来表示良性和恶性肿瘤。或者说是负样本和正样本现在我们不全部画<strong>X</strong>，良性的肿瘤改成用 <strong>O</strong> 表示，恶性的继续用 <strong>X</strong> 表示。来预测肿瘤的恶性与否。</p>
<p>在其它一些机器学习问题中，可能会遇到不止一种特征。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄。在其他机器学习问题中，我们通常有更多的特征，我朋友研究这个问题时，通常采用这些特征，比如肿块密度，肿瘤细胞尺寸的一致性和形状的一致性等等，还有一些其他的特征。这就是我们即将学到最有趣的学习算法之一。</p>
<p>那种算法不仅能处理2种3种或5种特征，即使有无限多种特征都可以处理。</p>
<p><img src="/images/image-20210216225452470.png" alt="image-20210216225452470"></p>
<p>上图中，我列举了总共5种不同的特征，坐标轴上的两种和右边的3种，但是在一些学习问题中，你希望不只用3种或5种特征。相反，你想用无限多种特征，好让你的算法可以利用大量的特征，或者说线索来做推测。那你怎么处理无限多个特征，甚至怎么存储这些特征都存在问题，你电脑的内存肯定不够用。<strong>我们以后会讲一个算法，叫支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。</strong>想象一下，我没有写下这两种和右边的三种特征，而是在一个无限长的列表里面，一直写一直写不停的写，写下无限多个特征，事实上，我们能用算法来处理它们。</p>
<p><strong>现在来回顾一下，这节课我们介绍了监督学习。其基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。我们还介绍了回归问题，即通过回归来推出一个连续的输出，之后我们介绍了分类问题，其目标是推出一组离散的结果。</strong></p>
<h3 id="1-3无监督学习"><a href="#1-3无监督学习" class="headerlink" title="1.3无监督学习"></a>1.3无监督学习</h3><p>本次视频中，我们将介绍第二种主要的机器学习问题。叫做无监督学习。</p>
<p><img src="/images/image-20210216225824075.png" alt="image-20210216225824075"></p>
<p>上个视频中，已经介绍了监督学习。回想当时的数据集，如图表所示，这个数据集中每条数据都已经标明是阴性或阳性，即是良性或恶性肿瘤。所以，对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案，是良性或恶性了。</p>
<p>在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，<strong>即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。</strong>所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。你能从数据中找到某种结构吗？针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。这是一个，那是另一个，二者不同。是的，<strong>无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法。</strong>事实证明，它能被用在很多地方。</p>
<h2 id="二-单变量线性回归-Linear-Regression-with-One-Variable"><a href="#二-单变量线性回归-Linear-Regression-with-One-Variable" class="headerlink" title="二.单变量线性回归(Linear Regression with One Variable)"></a>二.单变量线性回归(Linear Regression with One Variable)</h2><h3 id="2-1模型表示"><a href="#2-1模型表示" class="headerlink" title="2.1模型表示"></a>2.1模型表示</h3><p>我们的第一个学习算法是线性回归算法。在这段视频中，你会看到这个算法的概况，更重要的是你将会了解监督学习过程完整的流程。</p>
<p>让我们通过一个例子来开始：这个例子是预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。</p>
<p><img src="/images/image-20210216232328225.png" alt="image-20210216232328225"></p>
<p>它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个回归问题。回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格，同时，还有另一种最常见的监督学习方式，叫做分类问题，当我们想要预测离散的输出值，例如，我们正在寻找癌症肿瘤，并想要确定肿瘤是良性的还是恶性的，这就是0/1离散输出的问题。更进一步来说，在监督学习中我们有一个数据集，这个数据集被称训练集。</p>
<p><strong>我将在整个课程中用小写的$m$来表示训练样本的数目。</strong></p>
<p>以之前的房屋交易问题为例，假使我们回归问题的训练集（<strong>Training Set</strong>）如下表所示：</p>
<p><img src="/images/image-20210216232438960.png" alt="image-20210216232438960"></p>
<p>我们将要用来描述这个回归问题的标记如下:</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$m$ 代表训练集中实例的数量</span><br><span class="line"></span><br><span class="line">$x$  代表特征/输入变量</span><br><span class="line"></span><br><span class="line">$y$ 代表目标变量/输出变量</span><br><span class="line"></span><br><span class="line">$\left( x,y \right)$ 代表训练集中的实例</span><br><span class="line"></span><br><span class="line">$(&#123;&#123;x&#125;^&#123;(i)&#125;&#125;,&#123;&#123;y&#125;^&#123;(i)&#125;&#125;)$ 代表第$i$ 个观察实例</span><br><span class="line"></span><br><span class="line">$h$  代表学习算法的解决方案或函数也称为假设（<span class="strong">**hypothesis**</span>）</span><br></pre></td></tr></table></figure>
<p><img src="/images/image-20210216232605293.png" alt="image-20210216232605293"></p>
<p>这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格<br>我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写 $h$  表示。$h$  代表<strong>hypothesis</strong>(<strong>假设</strong>)，$h$表示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 $h$ 根据输入的 $x$值来得出 $y$ 值，$y$ 值对应房子的价格 因此，$h$ 是一个从$x$ 到 $y$ 的函数映射。</p>
<p>我将选择最初的使用规则$h$代表<strong>hypothesis</strong>，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设$h$，然后将我们要预测的房屋的尺寸作为输入变量输入给$h$，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 $h$？</p>
<p>一种可能的表达方式为：$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p>
<h3 id="2-2代价函数"><a href="#2-2代价函数" class="headerlink" title="2.2代价函数"></a>2.2代价函数</h3><p>在这段视频中我们将定义代价函数的概念，这有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合。如图：</p>
<p><img src="/images/image-20210216234140487.png" alt="image-20210216234140487"></p>
<p>在线性回归中我们有一个像这样的训练集，$m$代表了训练样本的数量，比如 $m = 47$。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：$h_\theta \left( x \right)=\theta_{0}+\theta_{1}x$。</p>
<p>接下来我们会引入一些术语我们现在要做的便是为我们的模型选择合适的<strong>参数</strong>（<strong>parameters</strong>）$\theta_{0}$ 和 $\theta_{1}$，在房价问题这个例子中便是直线的斜率和在$y$ 轴上的截距。</p>
<p>我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是<strong>建模误差</strong>（<strong>modeling error</strong>）。</p>
<p><img src="/images/image-20210216234256392.png" alt="image-20210216234256392"></p>
<p>我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数 $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$最小。</p>
<p>我们绘制一个等高线图，三个坐标分别为$\theta_{0}$和$\theta_{1}$ 和$J(\theta_{0}, \theta_{1})$：</p>
<p><img src="/images/image-20210216234547570.png" alt="image-20210216234547570"></p>
<p><strong>通过不断调节theta_{0}, theta_{1}的值，使代价函数的值最小，此时得到的theta_{0}, theta_{1}变是我们期望得到的值。</strong></p>
<p>则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点。</p>
<p>代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。</p>
<p>在后续课程中，我们还会谈论其他的代价函数，但我们刚刚讲的选择是对于大多数线性回归问题非常合理的。</p>
<p>也许这个函数$J(\theta_{0}, \theta_{1})$有点抽象，可能你仍然不知道它的内涵，在接下来的几个视频里，我们要更进一步解释代价函数J的工作原理，并尝试更直观地解释它在计算什么，以及我们使用它的目的。</p>
<h3 id="2-3代价函数的直观理解"><a href="#2-3代价函数的直观理解" class="headerlink" title="2.3代价函数的直观理解|"></a>2.3代价函数的直观理解|</h3><p>参考视频: 2 - 3 - Cost Function - Intuition I (11 min).mkv</p>
<p>在上一个视频中，我们给了代价函数一个数学上的定义。在这个视频里，让我们通过一些例子来获取一些直观的感受，看看代价函数到底是在干什么。</p>
<p><img src="/images/image-20210216235141468.png" alt="image-20210216235141468"></p>
<p><img src="/images/image-20210216235152322.png" alt="image-20210216235152322"></p>
<h3 id="2-4代价函数的直观理解"><a href="#2-4代价函数的直观理解" class="headerlink" title="2.4代价函数的直观理解||"></a>2.4代价函数的直观理解||</h3><p><img src="/images/image-20210216235528290.png" alt="image-20210216235528290"></p>
<p>代价函数的样子，等高线图，则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点。</p>
<p><img src="/images/image-20210216235547767.png" alt="image-20210216235547767"></p>
<p>通过这些图形，我希望你能更好地理解这些代价函数$ J$所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数$J$的最小值。</p>
<p>当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数$J$取最小值的参数$\theta_{0}$和$\theta_{1}$来。</p>
<p>我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的$\theta_{0}$和$\theta_{1}$的值，在下一节视频中，我们将介绍一种算法，能够自动地找出能使代价函数$J$最小化的参数$\theta_{0}$和$\theta_{1}$的值。</p>
<h3 id="2-5梯度下降"><a href="#2-5梯度下降" class="headerlink" title="2.5梯度下降"></a>2.5梯度下降</h3><p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_{0}, \theta_{1})$ 的最小值。</p>
<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合$\left( {\theta_{0}},{\theta_{1}},……,{\theta_{n}} \right)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（<strong>local minimum</strong>），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（<strong>global minimum</strong>），选择不同的初始参数组合，可能会找到不同的局部最小值。</p>
<p><img src="/images/image-20210216235758941.png" alt="image-20210216235758941"></p>
<p>想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。</p>
<p>批量梯度下降（<strong>batch gradient descent</strong>）算法的公式为：</p>
<p><img src="/images/image-20210217000026099.png" alt="image-20210217000026099"></p>
<p>其中$a$是学习率（<strong>learning rate</strong>），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。</p>
<p><img src="/images/image-20210217000338121.png" alt="image-20210217000338121"></p>
<p>在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们要更新${\theta_{0}}$和${\theta_{1}}$ ，当 $j=0$ 和$j=1$时，会产生更新，所以你将更新$J\left( {\theta_{0}} \right)$和$J\left( {\theta_{1}} \right)$。实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要同时更新${\theta_{0}}$和${\theta_{1}}$，我的意思是在这个等式中，我们要这样更新：</p>
<p>${\theta_{0}}$:= ${\theta_{0}}$ ，并更新${\theta_{1}}$:= ${\theta_{1}}$。</p>
<p>实现方法是：你应该计算公式右边的部分，通过那一部分计算出${\theta_{0}}$和${\theta_{1}}$的值，然后同时更新${\theta_{0}}$和${\theta_{1}}$。</p>
<p>让我进一步阐述这个过程：</p>
<p><img src="/images/image-20210217000401134.png" alt="image-20210217000401134"></p>
<p>在梯度下降算法中，这是正确实现同时更新的方法。我不打算解释为什么你需要同时更新，同时更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。</p>
<p>在接下来的视频中，我们要进入这个微分项的细节之中。我已经写了出来但没有真正定义，如果你已经修过微积分课程，如果你熟悉偏导数和导数，这其实就是这个微分项：</p>
<p><img src="/images/image-20210217002837122.png" alt="image-20210217002837122"></p>
<p>如果你不熟悉微积分，不用担心，即使你之前没有看过微积分，或者没有接触过偏导数，在接下来的视频中，你会得到一切你需要知道，如何计算这个微分项的知识。</p>
<p>下一个视频中，希望我们能够给出实现梯度下降算法的所有知识 。</p>
<h3 id="2-6梯度下降的直观理解"><a href="#2-6梯度下降的直观理解" class="headerlink" title="2.6梯度下降的直观理解"></a>2.6梯度下降的直观理解</h3><p>在之前的视频中，我们给出了一个数学上关于梯度下降的定义，本次视频我们更深入研究一下，更直观地感受一下这个算法是做什么的，以及梯度下降算法的更新过程有什么意义。梯度下降算法如下：</p>
<p>${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left(\theta \right)$</p>
<p>描述：对$\theta $赋值，使得$J\left( \theta  \right)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中$a$是学习率（<strong>learning rate</strong>），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。</p>
<p><img src="/images/image-20210217000534228.png" alt="image-20210217000534228"></p>
<p>对于这个问题，求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，让我们看看这条红色直线的斜率，就是这条刚好与函数曲线相切的这条直线，这条直线的斜率正好是这个三角形的高度除以这个水平长度，现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的${\theta_{1}}$，${\theta_{1}}$更新后等于${\theta_{1}}$减去一个正数乘以$a$。</p>
<p>这就是我梯度下降法的更新规则：${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left( \theta  \right)$</p>
<p>让我们来看看如果$a$太小或$a$太大会出现什么情况：</p>
<p>如果$a$太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果$a$太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。</p>
<p>如果$a$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果$a$太大，它会导致无法收敛，甚至发散。</p>
<p>现在，我还有一个问题，当我第一次学习这个地方时，我花了很长一段时间才理解这个问题，如果我们预先把${\theta_{1}}$放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？</p>
<p>假设你将${\theta_{1}}$初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得${\theta_{1}}$不再改变，也就是新的${\theta_{1}}$等于原来的${\theta_{1}}$，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率$a$保持不变时，梯度下降也可以收敛到局部最低点。</p>
<p>我们来看一个例子，这是代价函数$J\left( \theta  \right)$。</p>
<p><img src="/images/image-20210217000553809.png" alt="image-20210217000553809"></p>
<p>我想找到它的最小值，首先初始化我的梯度下降算法，在那个品红色的点初始化，如果我更新一步梯度下降，也许它会带我到这个点，因为这个点的导数是相当陡的。现在，在这个绿色的点，如果我再更新一步，你会发现我的导数，也即斜率，是没那么陡的。随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所以，我再进行一步梯度下降时，我的导数项是更小的，${\theta_{1}}$更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。</p>
<p>回顾一下，在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小$a$。</p>
<p>这就是梯度下降算法，你可以用它来最小化任何代价函数$J$，不只是线性回归中的代价函数$J$。</p>
<p>在接下来的视频中，我们要用代价函数$J$，回到它的本质，线性回归中的代价函数。也就是我们前面得出的平方误差函数，结合梯度下降法，以及平方代价函数，我们会得出第一个机器学习算法，即线性回归算法。</p>
<p><img src="/images/image-20210217002957464.png" alt="image-20210217002957464"></p>
<p><img src="/images/image-20210217003019388.png" alt="image-20210217003019388"></p>
<p><img src="/images/image-20210217003035133.png" alt="image-20210217003035133"></p>
<p>则算法改写成：</p>
<h3 id><a href="#" class="headerlink" title></a><img src="/images/image-20210217002906737.png" alt="image-20210217002906737"></h3><h3 id="2-7梯度下降算法更多内容"><a href="#2-7梯度下降算法更多内容" class="headerlink" title="2.7梯度下降算法更多内容"></a>2.7梯度下降算法更多内容</h3><p>请参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41800366/article/details/86583789?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161349126216780261954075%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=161349126216780261954075&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-86583789.first_rank_v2_pc_rank_v29&amp;utm_term=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1018.2226.3001.4187">链接</a></p>
<h2 id="三-线性代数回顾-Linear-Algebra-Review"><a href="#三-线性代数回顾-Linear-Algebra-Review" class="headerlink" title="三.线性代数回顾(Linear Algebra Review)"></a>三.线性代数回顾(Linear Algebra Review)</h2><h2 id="四-多变量线性回归-Linear-Regression-with-Multiple-Variables"><a href="#四-多变量线性回归-Linear-Regression-with-Multiple-Variables" class="headerlink" title="四.多变量线性回归(Linear Regression with Multiple Variables)"></a>四.多变量线性回归(Linear Regression with Multiple Variables)</h2><h3 id="4-1多维特征"><a href="#4-1多维特征" class="headerlink" title="4.1多维特征"></a>4.1多维特征</h3><p>目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为</p>
<script type="math/tex; mode=display">
\left( {x_{1}},{x_{2}},...,{x_{n}} \right)</script><p><img src="/images/image-20210217135149679.png" alt="image-20210217135149679"></p>
<p>增添更多特征后，我们引入一系列新的注释：</p>
<p>n代表特征的数量</p>
<p><img src="/images/image-20210217135313151.png" alt="image-20210217135313151">代表第 i 个训练实例，是特征矩阵中的第i行，是一个<strong>向量</strong>（<strong>vector</strong>）。</p>
<p>比方说，上图的</p>
<script type="math/tex; mode=display">
{x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}</script><p><img src="/images/image-20210217135736845.png" alt="image-20210217135736845">代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征。</p>
<p>如上图的<img src="/images/image-20210217135935284.png" alt="image-20210217135935284"></p>
<p>支持多变量的假设 h表示为：<img src="/images/image-20210217140013223.png" alt="image-20210217140013223"></p>
<p>这个公式中有n+1个参数和n个变量，为了使得公式能够简化一些，引入<img src="/images/image-20210217140116234.png" alt="image-20210217140116234">则公式转化为：<img src="/images/image-20210217140159797.png" alt="image-20210217140159797"></p>
<p>此时模型中的参数是一个n+1维的向量,任何一个训练实例也都是n+1维的向量,特征矩阵X的维度是m*(n+1).因此公式可以简化为：</p>
<script type="math/tex; mode=display">
h_{\theta} \left( x \right)={\theta^{T}}X</script><p>其中上标T代表矩阵转置。</p>
<h3 id="4-2-多变量梯度下降"><a href="#4-2-多变量梯度下降" class="headerlink" title="4.2 多变量梯度下降"></a>4.2 多变量梯度下降</h3><p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：</p>
<p><img src="/images/image-20210217141700534.png" alt="image-20210217141700534"></p>
<p>其中<img src="/images/image-20210217141916558.png" alt="image-20210217141916558"></p>
<p>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。<br>多变量线性回归的批量梯度下降算法为：</p>
<p><img src="/images/image-20210217141936436.png" alt="image-20210217141936436"></p>
<p>即：</p>
<p><img src="/images/image-20210217141947358.png" alt="image-20210217141947358"></p>
<p>当n&gt;=1时:</p>
<p><img src="/images/image-20210217142148319.png" alt="image-20210217142148319"></p>
<p><img src="/images/image-20210217142153897.png" alt="image-20210217142153897"></p>
<p><img src="/images/image-20210217142200423.png" alt="image-20210217142200423"></p>
<p>我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。</p>
<p><strong>Python</strong> 代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span>(<span class="params">X, y, theta</span>):</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(inner) / (<span class="number">2</span> * <span class="built_in">len</span>(X))</span><br></pre></td></tr></table></figure>
<h3 id="4-3梯度下降法实践1-特征缩放"><a href="#4-3梯度下降法实践1-特征缩放" class="headerlink" title="4.3梯度下降法实践1-特征缩放"></a>4.3梯度下降法实践1-特征缩放</h3><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p>
<p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</p>
<p><img src="/images/image-20210217144009670.png" alt="image-20210217144009670"></p>
<p>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图：</p>
<p><img src="/images/image-20210217144026104.png" alt="image-20210217144026104"></p>
<p>最简单的方法是令：</p>
<p><img src="/images/image-20210217144136166.png" alt="image-20210217144136166"></p>
<p>其中<img src="/images/image-20210217144228881.png" alt="image-20210217144228881">是平均值，<img src="/images/image-20210217144303006.png" alt="image-20210217144303006">是标准差</p>
<h3 id="4-4梯度下降法实践2-学习率"><a href="#4-4梯度下降法实践2-学习率" class="headerlink" title="4.4梯度下降法实践2-学习率"></a>4.4梯度下降法实践2-学习率</h3><p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。</p>
<p><img src="/images/image-20210217144426235.png" alt="image-20210217144426235"></p>
<p>也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较，但通常看上面这样的图表更好。</p>
<p>梯度下降算法的每次迭代受到学习率的影响，如果学习率$a$过小，则达到收敛所需的迭代次数会非常高；如果学习率$a$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试些学习率：</p>
<script type="math/tex; mode=display">
\alpha=0.01，0.03，0.1，0.3，1，3，10</script><h3 id="4-5-特征和多项式回归"><a href="#4-5-特征和多项式回归" class="headerlink" title="4.5 特征和多项式回归"></a>4.5 特征和多项式回归</h3><p>如房价预测问题，</p>
<p><img src="/images/image-20210218161127888.png" alt="image-20210218161127888"></p>
<p><img src="/images/image-20210218161153908.png" alt="image-20210218161153908"></p>
<p><img src="/images/image-20210218161321350.png" alt="image-20210218161321350"></p>
<p><img src="/images/image-20210218161337520.png" alt="image-20210218161337520"></p>
<p>则：</p>
<p><img src="/images/image-20210218161351222.png" alt="image-20210218161351222"></p>
<p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：</p>
<p><img src="/images/image-20210218161449597.png" alt="image-20210218161449597"></p>
<p>或者三次方模型:</p>
<p><img src="/images/image-20210218161519095.png" alt="image-20210218161519095"></p>
<p><img src="/images/image-20210218161533113.png" alt="image-20210218161533113"></p>
<p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：<img src="/images/image-20210218161643826.png" alt="image-20210218161643826">从而将模型转化为线性回归模型。</p>
<p>根据函数图形特性，我们还可以使：<img src="/images/image-20210218161726641.png" alt="image-20210218161726641"></p>
<p>或者:<img src="/images/image-20210218161742466.png" alt="image-20210218161742466"></p>
<p>注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</p>
<h3 id="4-6正规方程"><a href="#4-6正规方程" class="headerlink" title="4.6正规方程"></a>4.6正规方程</h3><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：</p>
<p><img src="/images/image-20210218162307180.png" alt="image-20210218162307180"></p>
<p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：</p>
<p><img src="/images/image-20210218162409718.png" alt="image-20210218162409718"></p>
<p>假设我们的训练集特征矩阵为X(包含了x0=1)并且我们的训练集结果为向量y,则利用正规方程解出向量</p>
<p><img src="/images/image-20210218162601709.png" alt="image-20210218162601709"></p>
<p>设<img src="/images/image-20210218162658334.png" alt="image-20210218162658334">则：<img src="/images/image-20210218162722849.png" alt="image-20210218162722849"></p>
<p><img src="/images/image-20210218162755926.png" alt="image-20210218162755926"></p>
<p><img src="/images/image-20210218162804677.png" alt="image-20210218162804677"></p>
<p>运用正规方程方法求解参数：</p>
<p><img src="/images/image-20210218162820290.png" alt="image-20210218162820290"></p>
<p>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。</p>
<p>梯度下降与正规方程的比较：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率$\alpha$</td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>当特征数量$n$大时也能较好适用</td>
<td>需要计算<img src="/images/image-20210218162916440.png" alt="image-20210218162916440"> 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为<img src="/images/image-20210218162943804.png" alt="image-20210218162943804">，通常来说当$n$小于10000 时还是可以接受的</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
</tbody>
</table>
</div>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<p>正规方程的python 实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">    </span><br><span class="line">   theta = np.linalg.inv(X.T@X)@X.T@y <span class="comment">#X.T@X等价于X.T.dot(X)</span></span><br><span class="line">    </span><br><span class="line">   <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>
<h3 id="4-7正规方程的不可逆过程"><a href="#4-7正规方程的不可逆过程" class="headerlink" title="4.7正规方程的不可逆过程"></a>4.7正规方程的不可逆过程</h3><h2 id="四-逻辑回归-Logistic-Regression"><a href="#四-逻辑回归-Logistic-Regression" class="headerlink" title="四.逻辑回归(Logistic Regression)"></a>四.逻辑回归(Logistic Regression)</h2><h3 id="6-1分类问题"><a href="#6-1分类问题" class="headerlink" title="6.1分类问题"></a>6.1分类问题</h3><p>在这个以及接下来的几个视频中，开始介绍分类问题。</p>
<p>在分类问题中，你要预测的变量 $y$ 是离散的值，我们将学习一种叫做逻辑回归 (<strong>Logistic Regression</strong>) 的算法，这是目前最流行使用最广泛的一种学习算法。</p>
<p>在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。</p>
<p>我们从二元的分类问题开始讨论。</p>
<p>我们将因变量(<strong>dependent variable</strong>)可能属于的两个类分别称为负向类（<strong>negative class</strong>）和正向类（<strong>positive class</strong>），则因变量$y\in { 0,1 \\}$ ，其中 0 表示负向类，1 表示正向类。</p>
<p><img src="/images/image-20210219205008736.png" alt="image-20210219205008736"></p>
<p><img src="/images/image-20210219205031615.png" alt="image-20210219205031615"></p>
<p>如果我们要用线性回归算法来解决一个分类问题，对于分类， $y$ 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签  $y$ 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。</p>
<p>顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签  $y$ 取值离散的情况，如：1 0 0 1。</p>
<p>在接下来的视频中，我们将开始学习逻辑回归算法的细节。</p>
<h3 id="6-2假说表示"><a href="#6-2假说表示" class="headerlink" title="6.2假说表示"></a>6.2假说表示</h3><p>在这段视频中，我要给你展示假设函数的表达式，也就是说，在分类问题中，要用什么样的函数来表示我们的假设。此前我们说过，希望我们的分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。</p>
<p>回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线：</p>
<p><img src="/images/image-20210219205335962.png" alt="image-20210219205335962"></p>
<p>根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测：</p>
<p><img src="/images/image-20210219205424685.png" alt="image-20210219205424685"></p>
<p><img src="/images/image-20210219205431707.png" alt="image-20210219205431707"></p>
<p>对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。</p>
<p><img src="/images/image-20210219205839015.png" alt="image-20210219205839015"></p>
<p>这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决这样的问题。</p>
<p>我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。</p>
<p>逻辑回归模型的假设是： </p>
<p><img src="/images/image-20210219205921075.png" alt="image-20210219205921075"></p>
<p>$X$ 代表特征向量<br>$g$ 代表逻辑函数（<strong>logistic function</strong>)是一个常用的逻辑函数为<strong>S</strong>形函数（<strong>Sigmoid function</strong>），公式为：<img src="/images/image-20210219205956276.png" alt="image-20210219205956276"></p>
<p><strong>python</strong>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    </span><br><span class="line">   <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br></pre></td></tr></table></figure>
<p>该函数的图像为：</p>
<p><img src="/images/image-20210219210047848.png" alt="image-20210219210047848"></p>
<p>合起来，我们得到逻辑回归模型的假设：</p>
<p>对模型的理解：<img src="/images/image-20210219210219689.png" alt="image-20210219210219689"></p>
<p>的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性,即<img src="/images/image-20210219210322856.png" alt="image-20210219210322856"></p>
<p>例如，如果对于给定的$x$，通过已经确定的参数计算得出</p>
<p><img src="/images/image-20210219213349970.png" alt="image-20210219213349970"></p>
<p>则表示有70%的几率$y$为正向类，相应地$y$为负向类的几率为1-0.7=0.3。</p>
<h3 id="6-3-判定边界"><a href="#6-3-判定边界" class="headerlink" title="6.3 判定边界"></a>6.3 判定边界</h3><p>现在讲下决策边界(<strong>decision boundary</strong>)的概念。这个概念能更好地帮助我们理解逻辑回归的假设函数在计算什么。</p>
<p><img src="/images/image-20210219223428799.png" alt="image-20210219223428799"></p>
<p>在逻辑回归中，我们预测：</p>
<p>当${h_\theta}\left( x \right)&gt;=0.5$时，预测 $y=1$。</p>
<p>当${h_\theta}\left( x \right)&lt;0.5$时，预测 $y=0$ 。</p>
<p>根据上面绘制出的 <strong>S</strong> 形函数图像，我们知道当</p>
<p>$z=0$ 时 $g(z)=0.5$</p>
<p>$z&gt;0$ 时 $g(z)&gt;0.5$</p>
<p>$z&lt;0$ 时 $g(z)&lt;0.5$</p>
<p>又 $z={\theta^{T}}x$ ，即：<br>${\theta^{T}}x&gt;=0$  时，预测 $y=1$<br>${\theta^{T}}x&lt;0$  时，预测 $y=0$</p>
<p>现在假设我们有一个模型：</p>
<p><img src="/images/image-20210219223559321.png" alt="image-20210219223559321"></p>
<p>并且参数$\theta$ 是向量[-3 1 1]。 则当<img src="/images/image-20210219223750629.png" alt="image-20210219223750629">模型将预测 $y=1$。</p>
<p>我们可以绘制直线${x_1}+{x_2} = 3$，这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。</p>
<p><img src="/images/image-20210219223822684.png" alt="image-20210219223822684"></p>
<p>假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？</p>
<p><img src="/images/image-20210219223838732.png" alt="image-20210219223838732"></p>
<p>因为需要用曲线才能分隔 $y=0$ 的区域和 $y=1$ 的区域，我们需要二次方特征：<img src="/images/image-20210219223947040.png" alt="image-20210219223947040"></p>
<p>是[-1 0 0 1 1]，则我们得到的判定边界恰好是圆点在原点且半径为1的圆形。</p>
<p>我们可以用非常复杂的模型来适应非常复杂形状的判定边界。</p>
<h3 id="6-4代价函数"><a href="#6-4代价函数" class="headerlink" title="6.4代价函数"></a>6.4代价函数</h3><p>在这段视频中，我们要介绍如何拟合逻辑回归模型的参数$\theta$。具体来说，我要定义用来拟合参数的优化目标或者叫代价函数，这便是监督学习问题中的逻辑回归模型的拟合问题。</p>
<p><img src="/images/image-20210220072049273.png" alt="image-20210220072049273"></p>
<p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将<img src="/images/image-20210220072158178.png" alt="image-20210220072158178">带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（<strong>non-convexfunction</strong>）。</p>
<p><img src="/images/image-20210220072214334.png" alt="image-20210220072214334"></p>
<p>这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p>
<p>线性回归的代价函数为：<img src="/images/image-20210220072332215.png" alt="image-20210220072332215">我们重新定义逻辑回归的代价函数为：<img src="/images/image-20210220072402181.png" alt="image-20210220072402181"></p>
<p>其中<img src="/images/image-20210220072422304.png" alt="image-20210220072422304"></p>
<p><img src="/images/image-20210220072449078.png" alt="image-20210220072449078">之间的关系如下图所示：<img src="/images/image-20210220072503337.png" alt="image-20210220072503337"></p>
<p>这样构建的<img src="/images/image-20210220072549402.png" alt="image-20210220072549402">函数的特点是：当实际的  $y=1$ 且<img src="/images/image-20210220072641331.png" alt="image-20210220072641331">也为 1 时误差为 0</p>
<p>当 $y=1$ 但<img src="/images/image-20210220072807366.png" alt="image-20210220072807366">不为1时误差随着<img src="/images/image-20210220072846545.png" alt="image-20210220072846545"></p>
<p>变小而变大；</p>
<p>当实际的 $y=0$ 且<img src="/images/image-20210220073104414.png" alt="image-20210220073104414">也为 0 时代价为 0，当$y=0$ 但<img src="/images/image-20210220073132499.png" alt="image-20210220073132499">不为 0时误差随着<img src="/images/image-20210220073214585.png" alt="image-20210220073214585">的变大而变大。将构建的<img src="/images/image-20210220073336344.png" alt="image-20210220073336344">简化如下：</p>
<p><img src="/images/image-20210220073534877.png" alt="image-20210220073534877"></p>
<p>带入代价函数得到：</p>
<p><img src="/images/image-20210220073624074.png" alt="image-20210220073624074"></p>
<p>即：<img src="/images/image-20210220073636614.png" alt="image-20210220073636614"></p>
<p><strong>Python</strong>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    </span><br><span class="line">  theta = np.matrix(theta)</span><br><span class="line">  X = np.matrix(X)</span><br><span class="line">  y = np.matrix(y)</span><br><span class="line">  first = np.multiply(-y, np.log(sigmoid(X* theta.T)))</span><br><span class="line">  second = np.multiply((<span class="number">1</span> - y), np.log(<span class="number">1</span> - sigmoid(X* theta.T)))</span><br><span class="line">  <span class="keyword">return</span> np.<span class="built_in">sum</span>(first - second) / (<span class="built_in">len</span>(X))</span><br></pre></td></tr></table></figure>
<p>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：</p>
<p><img src="/images/image-20210220074848320.png" alt="image-20210220074848320"></p>
<p>求导后得到：</p>
<p><img src="/images/image-20210220074927936.png" alt="image-20210220074927936"></p>
<p>在这个视频中，我们定义了单训练样本的代价函数，凸性分析的内容是超出这门课的范围的，但是可以证明我们所选的代价值函数会给我们一个凸优化问题。代价函数$J(\theta)$会是一个凸函数，并且没有局部最优值。</p>
<p>推导过程：</p>
<p><img src="/images/image-20210220075005570.png" alt="image-20210220075005570"></p>
<p>考虑：<img src="/images/image-20210220075041803.png" alt="image-20210220075041803">则：<img src="/images/image-20210220075116773.png" alt="image-20210220075116773"></p>
<p><img src="/images/image-20210220075154821.png" alt="image-20210220075154821"></p>
<p><img src="/images/image-20210220075217278.png" alt="image-20210220075217278"></p>
<p>所以：</p>
<p><img src="/images/image-20210220075622898.png" alt="image-20210220075622898"></p>
<p>注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的<img src="/images/image-20210220075658606.png" alt="image-20210220075658606">与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。</p>
<p>一些梯度下降算法之外的选择：<br>除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：<strong>共轭梯度</strong>（<strong>Conjugate Gradient</strong>），<strong>局部优化法</strong>(<strong>Broyden fletcher goldfarb shann,BFGS</strong>)和<strong>有限内存局部优化法</strong>(<strong>LBFGS</strong>) .</p>
<h3 id="6-5-简化的成本函数和梯度下降"><a href="#6-5-简化的成本函数和梯度下降" class="headerlink" title="6.5 简化的成本函数和梯度下降"></a>6.5 简化的成本函数和梯度下降</h3>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/MachineLearning/" rel="tag"># MachineLearning</a>

 <!-- 文章结束表示语-->
    <div>
      
        <div>
    
        <div style="text-align:center;color: #636363;font-size:14px;letter-spacing: 10px">本文结束啦<i class="fa fa-bell"></i>感谢您的阅读</div>
    
</div>
      
    </div>


          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/15/%E5%9B%B4%E5%9F%8E/" rel="prev" title="围城">
      <i class="fa fa-chevron-left"></i> 围城
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/01/17/opencv%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="next" title="opencv学习笔记">
      opencv学习笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
    <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

<embed id="musicplayer" frameborder="no" border="0" marginwidth="0" marginheight="0" width=298 height=52 src="//music.163.com/outchain/player?type=2&id=33756016&auto=1&height=32"></embed>
  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80-%E5%BC%95%E8%A8%80%EF%BC%88Introduction"><span class="nav-text">一.引言（Introduction)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-text">1.1什么是机器学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">1.2监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">1.3无监督学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C-%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-Linear-Regression-with-One-Variable"><span class="nav-text">二.单变量线性回归(Linear Regression with One Variable)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA"><span class="nav-text">2.1模型表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-text">2.2代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-text">2.3代价函数的直观理解|</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-text">2.4代价函数的直观理解||</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">2.5梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-text">2.6梯度下降的直观理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text"></span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E6%9B%B4%E5%A4%9A%E5%86%85%E5%AE%B9"><span class="nav-text">2.7梯度下降算法更多内容</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9B%9E%E9%A1%BE-Linear-Algebra-Review"><span class="nav-text">三.线性代数回顾(Linear Algebra Review)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B-%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-Linear-Regression-with-Multiple-Variables"><span class="nav-text">四.多变量线性回归(Linear Regression with Multiple Variables)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81"><span class="nav-text">4.1多维特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E5%A4%9A%E5%8F%98%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">4.2 多变量梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B51-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="nav-text">4.3梯度下降法实践1-特征缩放</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B52-%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-text">4.4梯度下降法实践2-学习率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-text">4.5 特征和多项式回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="nav-text">4.6正规方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-7%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E7%9A%84%E4%B8%8D%E5%8F%AF%E9%80%86%E8%BF%87%E7%A8%8B"><span class="nav-text">4.7正规方程的不可逆过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-Logistic-Regression"><span class="nav-text">四.逻辑回归(Logistic Regression)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-text">6.1分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2%E5%81%87%E8%AF%B4%E8%A1%A8%E7%A4%BA"><span class="nav-text">6.2假说表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C"><span class="nav-text">6.3 判定边界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-text">6.4代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-%E7%AE%80%E5%8C%96%E7%9A%84%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">6.5 简化的成本函数和梯度下降</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="grit"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">grit</p>
  <div class="site-description" itemprop="description">身体和灵魂总要有一个在路上</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/xiangruiA" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xiangruiA" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/weixin_46108138" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_46108138" rel="noopener" target="_blank"><i class="book fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=2543344628@qq.com" title="E - Mail → https:&#x2F;&#x2F;mail.qq.com&#x2F;cgi-bin&#x2F;qm_share?t&#x3D;qm_mailme&amp;email&#x3D;2543344628@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E - Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">grit</span>

    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">75k</span>
    <div class="powered-by">
    <i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
       本站访客数:<span id="busuanzi_value_site_uv"></span>
    </span>
    </div>

   <span class="post-meta-divider">|</span>
   
   <span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
   </span>


   

    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:08</span>
</div>

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<span id="sitetime"></span>
<script language=javascript>
	function siteTime(){
		window.setTimeout("siteTime()", 1000);
		var seconds = 1000;
		var minutes = seconds * 60;
		var hours = minutes * 60;
		var days = hours * 24;
		var years = days * 365;
		var today = new Date();
		var todayYear = today.getFullYear();
		var todayMonth = today.getMonth()+1;
		var todayDate = today.getDate();
		var todayHour = today.getHours();
		var todayMinute = today.getMinutes();
		var todaySecond = today.getSeconds();
		/* 
		Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
		year - 作为date对象的年份，为4位年份值
		month - 0-11之间的整数，做为date对象的月份
		day - 1-31之间的整数，做为date对象的天数
		hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
		minutes - 0-59之间的整数，做为date对象的分钟数
		seconds - 0-59之间的整数，做为date对象的秒数
		microseconds - 0-999之间的整数，做为date对象的毫秒数
        */
		var t1 = Date.UTC(2020,02,13,15,00,00); //北京时间2018-2-13 00:00:00
		var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
		var diff = t2-t1;
		var diffYears = Math.floor(diff/years);
		var diffDays = Math.floor((diff/days)-diffYears*365);
		var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
		var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
		var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
		document.getElementById("sitetime").innerHTML=" 已运行"+/*diffYears+" 年 "+*/diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
	}
	siteTime();
</script>


        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qzNLqU1UIaQIMaCBaFi4SaRM-gzGzoHsz',
      appKey     : 'vYdThgQ0euJt90NlkPCUkIBY',
      placeholder: "欢迎留言",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

    </div>
</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>